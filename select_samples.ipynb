{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info-Coevolution\n",
    "In this notebook, we will go through the usage of Info-Coevolution in classification tasks. \n",
    "\n",
    "Info-Coevolution is using HNSW as ANN search engine, while other options are also possible.\n",
    "\n",
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_index # implementation of main code in build_index (data structure and Bayesian Fusion)\n",
    "from build_index import confidence_convergence\n",
    "import numpy as np\n",
    "import torch\n",
    "import types\n",
    "from tqdm import tqdm\n",
    "# import your other dependencies\n",
    "\n",
    "dataset = None #<TODO:load_your_dataset_here> # the mixed labeled and unlabeled data\n",
    "data_loader = None #<TODO:init_your_dataloader_here> \n",
    "model = None #<TODO:load_your_model_here. Model should be trained with some initial data (like 1%~10%).>\n",
    "embedding_dim = None #<TODO:set your model embedding dim here>\n",
    "data_len = len(dataset)\n",
    "device = None #<TODO: set your device here>\n",
    "num_classes = 10\n",
    "\n",
    "ann_index = build_index.Singlemodal_index(dim=embedding_dim,n=data_len,submodular_k=16,num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "**Note: choose one of (a) and (b) depending on your model and data loading.**\n",
    "\n",
    "(a) If you want to embed your code to those codebase where model is wrapped in some class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann_index(self):\n",
    "    \"\"\"\n",
    "    Inference on all data, and build up the ANN index with sample embeddings and \n",
    "    initial labels/pseudo-labels with confidence values.\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "    data_loader = self.data_loader # dataloader with both \n",
    "    # total_loss = 0.0\n",
    "    # total_num = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "    y_logits = []\n",
    "    confidence_list = []\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(data_loader):\n",
    "            if isinstance(x, dict):\n",
    "                x = {k: v.to(device) for k, v in x.items()}\n",
    "            else:\n",
    "                x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            num_batch = y.shape[0]\n",
    "            total_num += num_batch\n",
    "\n",
    "            out = self.model(x) # Here in this sample, out is a dic with {'feat':embedding, 'logits', logits}\n",
    "            logits = out['logits'] # TODO: let your model output logits somewhere and use here\n",
    "            prob = torch.softmax(logits, dim=-1)\n",
    "            feat = out['feat'].detach().cpu()\n",
    "            pred = torch.max(logits, dim=-1)[1].cpu()\n",
    "            conf = (prob.max(dim=-1)[0]-1./num_classes)/(1-1./num_classes)\n",
    "            \n",
    "            for f,label,confid in zip(feat, pred.detach().cpu(),conf.detach().cpu()):\n",
    "                ann_index.add_item(build_index.DataPoint(None,f,label,confid))\n",
    "            # loss = F.cross_entropy(logits, y, reduction='mean', ignore_index=-1) # print total_loss/total_num to debug\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "            y_logits.append(logits.cpu().numpy())\n",
    "            y_probs.extend(prob.cpu().tolist())\n",
    "            # total_loss += loss.item() * num_batch    ## print total_loss/total_num to debug\n",
    "            confidence_list.extend(conf.cpu().tolist())\n",
    "    \n",
    "    return confidence_list, y_true, y_pred\n",
    "\n",
    "model.build_ann_index = types.MethodType(build_ann_index,model)\n",
    "print('building ann index')\n",
    "conf, y_t, y_p = model.build_ann_index()\n",
    "print('ann_index built')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (b) **Or** if your model is not wrapped and can be directly used in script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss = 0.0\n",
    "total_num = 0.0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_probs = []\n",
    "y_logits = []\n",
    "confidence_list = []\n",
    "with torch.no_grad():\n",
    "    for x,y in tqdm(data_loader):\n",
    "        if isinstance(x, dict):\n",
    "            x = {k: v.to(device) for k, v in x.items()}\n",
    "        else:\n",
    "            x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        num_batch = y.shape[0]\n",
    "        total_num += num_batch\n",
    "\n",
    "        out = model(x) # Here in this sample, out is a dic with {'feat':embedding, 'logits', logits}\n",
    "        logits = out['logits'] # TODO: let your model output logits somewhere and use here\n",
    "        prob = torch.softmax(logits, dim=-1)\n",
    "        feat = out['feat'].detach().cpu()\n",
    "        pred = torch.max(logits, dim=-1)[1].cpu()\n",
    "        conf = (prob.max(dim=-1)[0]-1./num_classes)/(1-1./num_classes)\n",
    "        \n",
    "        for f,label,confid in zip(feat, pred.detach().cpu(),conf.detach().cpu()):\n",
    "            ann_index.add_item(build_index.DataPoint(None,f,label,confid))\n",
    "        # loss = F.cross_entropy(logits, y, reduction='mean', ignore_index=-1)\n",
    "        y_true.extend(y.cpu().tolist())\n",
    "        y_pred.extend(pred.tolist())\n",
    "        y_logits.append(logits.cpu().numpy())\n",
    "        y_probs.extend(prob.cpu().tolist())\n",
    "        # total_loss += loss.item() * num_batch\n",
    "        confidence_list.extend(conf.cpu().tolist())\n",
    "conf, y_t, y_p = confidence_list, y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reannotate_gain(relabel_confidence, use_ann_index=False):\n",
    "    #Try directly using confidence instead of making it into entropy, so that the cosine space is more reasonable\n",
    "    gain_list = np.zeros(data_len)\n",
    "    c_list = np.zeros(data_len)\n",
    "    p_list = np.zeros(data_len)\n",
    "    for idx in tqdm(range(data_len)):\n",
    "        if ann_index.data[idx].confidence<relabel_confidence:\n",
    "            if use_ann_index and ann_index is not None:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx], k=8, skip_one=True)\n",
    "                preds_model = (ann_index.data[idx].label, ann_index.data[idx].confidence)\n",
    "                merged_p,c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                p_list[idx] = merged_p\n",
    "\n",
    "            else:\n",
    "                c = ann_index.data[idx].confidence\n",
    "                p_list[idx] = ann_index.data[idx].label\n",
    "\n",
    "            gain_list[idx] = max(0, relabel_confidence-c)\n",
    "            c_list[idx] = c\n",
    "        else:\n",
    "            c_list[idx] = 1\n",
    "            p_list[idx] = ann_index.data[idx].label\n",
    "            continue\n",
    "    # here return positive gains in ablation\n",
    "    return gain_list, c_list, p_list\n",
    "\n",
    "expected_confidence=1\n",
    "full_gain,c_list, p_list = reannotate_gain(expected_confidence,True)\n",
    "# print('non zero gain number',len(np.nonzero(gain)[0]))  # debugging print\n",
    "# print(sorted(gain[np.nonzero(gain)[0]].tolist()))   # debugging print\n",
    "non_zero_gain_idx = np.nonzero(full_gain)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Selection with Dynamic Rechecking\n",
    "We have single label mode and batch mode. In our in-house data setting, batch mode with a larger batch (10k samples per step) works better than small batch (100 and 1k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_samples = None #<TODO: put your labeled sample index here>\n",
    "full_gain[selected_samples]=0\n",
    "target_number = 128117 #<TODO: put your target annotation number here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Single label mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "count = 0\n",
    "labeled_set = set(selected_samples)\n",
    "return_list = set()\n",
    "gain = full_gain.copy()\n",
    "sum_gain = sum(gain)\n",
    "start = time.time()\n",
    "stop_criterion = 1 # use this to control automatic stop. If estimated remaining total confidence gain < 1, we stop.\n",
    "while count<target_number and sum_gain>stop_criterion:\n",
    "    # print(sum(gain),sum_gain)\n",
    "    idx = np.random.choice(data_len,p=gain/sum(gain))\n",
    "    sum_gain -= gain[idx]\n",
    "    gain[idx] = 0\n",
    "    if idx in return_list or idx in labeled_set: \n",
    "        continue\n",
    "    \n",
    "    relabel_y = y_t[idx] # TODO: Check your logic: here we use true label for research simulation/verification. \n",
    "                         # For real usage, substitute api or generate the sample indices for annotation.\n",
    "    \n",
    "    ann_index.data[idx].label = relabel_y\n",
    "    ann_index.data[idx].confidence = 1\n",
    "    return_list.add(idx)\n",
    "    \n",
    "    # Dynamic rechecking\n",
    "    I_near_labels, I_near_distances = ann_index.k_nearest_neighbour_I(ann_index.data[idx], 8, skip_one=True)\n",
    "    selected_ids = I_near_labels[I_near_distances<=0.1]\n",
    "    sim = 1-I_near_distances[I_near_distances<=0.1]\n",
    "    classes = np.array([ann_index.data[idx].label for idx in selected_ids])\n",
    "    # confidences = np.array([ann_index.data[idx].confidence for idx in selected_ids])\n",
    "    \n",
    "    for idx_neigh,s, cls in zip(selected_ids,sim,classes):\n",
    "        if cls==relabel_y:\n",
    "            if s>0.85:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                new_gain = max(0, 1-new_c)\n",
    "                sum_gain += (new_gain-gain[idx_neigh])\n",
    "                gain[idx_neigh] = new_gain\n",
    "        else:\n",
    "            if s>0.85:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                new_gain = max(gain[idx_neigh], 1-new_c)\n",
    "                sum_gain += (new_gain-gain[idx_neigh])\n",
    "                gain[idx_neigh] = new_gain\n",
    "                # print('found conflicting neighbour!')\n",
    "    count+=1\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Batch Labelling Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann作为初始集：\n",
    "import time\n",
    "selection_batchsize = 100 # <TODO: Adjust according to your data.>\n",
    "count = 0\n",
    "labeled_set = set(selected_samples)\n",
    "return_list = set()\n",
    "gain = full_gain.copy()\n",
    "sum_gain = sum(gain)\n",
    "stop_criterion = 1 # use this to control automatic stop. If estimated remaining total confidence gain < 1, we stop.\n",
    "start = time.time()\n",
    "while count<target_number and sum_gain>stop_criterion:\n",
    "    \n",
    "    # print(sum(gain),sum_gain)\n",
    "    \n",
    "    idxs = np.random.choice(50000,min(selection_batchsize,target_number-count),p=gain/sum(gain),replace=False)\n",
    "    keep = []\n",
    "    for idx in idxs:\n",
    "        sum_gain -= gain[idx]\n",
    "        gain[idx] = 0\n",
    "        if idx in return_list or idx in labeled_set: \n",
    "            continue\n",
    "        else:\n",
    "            keep.append(idx)\n",
    "            \n",
    "    \n",
    "    for idx in keep:\n",
    "        relabel_y = y_t[idx]\n",
    "        ann_index.data[idx].label = relabel_y\n",
    "        ann_index.data[idx].confidence = 1\n",
    "        return_list.add(idx)\n",
    "\n",
    "        I_near_labels, I_near_distances = ann_index.k_nearest_neighbour_I(ann_index.data[idx], 8, skip_one=True)\n",
    "        selected_ids = I_near_labels[I_near_distances<=0.1]\n",
    "        sim = 1-I_near_distances[I_near_distances<=0.1]\n",
    "        classes = np.array([ann_index.data[idx].label for idx in selected_ids])\n",
    "        # confidences = np.array([ann_index.data[idx].confidence for idx in selected_ids])\n",
    "\n",
    "        for idx_neigh,s, cls in zip(selected_ids,sim,classes):\n",
    "            if cls==relabel_y:\n",
    "                if s>0.85:\n",
    "                    preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                    preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                    new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                    new_gain = max(0, 1-new_c)\n",
    "                    sum_gain += (new_gain-gain[idx_neigh])\n",
    "                    gain[idx_neigh] = new_gain\n",
    "            else:\n",
    "                if s>0.85:\n",
    "                    preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                    preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                    new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                    new_gain = max(gain[idx_neigh], 1-new_c)\n",
    "                    sum_gain += (new_gain-gain[idx_neigh])\n",
    "                    gain[idx_neigh] = new_gain\n",
    "                    # print('found conflicting neighbour!')\n",
    "    count+=len(keep)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For extending semi-supervised unlabeled data\n",
    "We can use this part of code when the target number is not achieved and we still want more samples, or when we want to estimate semi-supervised data for annotation.\n",
    "\n",
    "Equation is: gain=dis_to_labeled + dis_to_selected_high_conf_unlabeled. that is to say, once we get a batch of midpoints, then we uniformly expand the boundarys. One way is to add selected_unlabeled into distance index and estimate the gain.\n",
    "\n",
    "Another point: for samples with confidence higher than 0.5 but lower than 0.9 acc, they are pesudo labeled actually. We may need these samples for middle stage learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_k = 4 # hyp to control neighbour number to estimate the distance gain\n",
    "emb_list = np.array([ann_index.data[idx].I_feature for idx in non_zero_gain_idx])\n",
    "labeled_set = set(selected_samples) # change it to 10000\n",
    "labeled_index = build_index.Singlemodal_index(dim=embedding_dim,n=data_len,submodular_k=16,num_classes=num_classes)\n",
    "conf = np.array(conf)\n",
    "high_conf_set = np.where(conf>0.9)[0]\n",
    "cleaner_set = set(high_conf_set).union(labeled_set)\n",
    "for id in cleaner_set:\n",
    "    labeled_index.add_item(build_index.DataPoint(None,emb_list[id],None,0)) # 扩展集数据的embeding和对应index注意处理，可以拼接起来\n",
    "dis_gain = np.zeros(data_len)\n",
    "for id in (set(range(data_len))-cleaner_set):\n",
    "    l,dis = labeled_index.k_nearest_neighbour_I(emb_list[id],average_k)\n",
    "    dis_gain[id] = np.mean(dis)\n",
    "dis_gain = dis_gain/(np.max(dis_gain)+1e-9) # normlize the impact of this gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# ann作为初始集：\n",
    "lamb = 1 # the hyp to control balance of distance gain and confidence gain. For extended dataset, \n",
    "         # we do not need to tune this as previous experiments show that lamb=1 is more stable for semi-supervised setting.\n",
    "gamma = 1 # in our tested setting, both being 1 is the best for semi-supervised data.\n",
    "count = 0\n",
    "return_list = set()\n",
    "gain = full_gain.copy()\n",
    "selection_batchsize = 100\n",
    "### For semi-supervised learning with threshold 0.5, this line emphasize those semi-labeled data. \n",
    "gain[gain>0.5] = 0 \n",
    "### Try using this line and without this line. NOTE: For extending supervised data, comment it out.\n",
    "\n",
    "gain = gamma * gain + lamb * dis_gain\n",
    "sum_gain = sum(gain)\n",
    "start = time.time()\n",
    "target_count = 500\n",
    "while count<target_count and sum_gain>1:\n",
    "    # print(sum(gain),sum_gain)\n",
    "    idxs = np.random.choice(50000,min(selection_batchsize,target_count-count),p=gain/sum(gain),replace=False)\n",
    "    keep = []\n",
    "    for idx in idxs:\n",
    "        sum_gain -= gain[idx]\n",
    "        gain[idx] = 0\n",
    "        if idx in return_list or idx in labeled_set: \n",
    "            continue\n",
    "        else:\n",
    "            keep.append(idx)\n",
    "    \n",
    "    for idx in keep:\n",
    "        relabel_y = y_t[idx] # TODO: check your logic. If doing annotation selection instead of selection, \n",
    "                             # send the keep list with deduplication for annotation.\n",
    "        ann_index.data[idx].label = relabel_y\n",
    "        ann_index.data[idx].confidence = 1\n",
    "        return_list.add(idx)\n",
    "\n",
    "        I_near_labels, I_near_distances = ann_index.k_nearest_neighbour_I(ann_index.data[idx], 8, skip_one=True)\n",
    "        selected_ids = I_near_labels[I_near_distances<=0.15]\n",
    "        sim = 1-I_near_distances[I_near_distances<=0.15]\n",
    "        # classes = np.array([ann_index.data[idx].label for idx in selected_ids])\n",
    "        # confidences = np.array([ann_index.data[idx].confidence for idx in selected_ids])\n",
    "\n",
    "        labeled_index.add_item(build_index.DataPoint(None,emb_list[id],None,0))\n",
    "        \n",
    "        for idx_neigh,s in zip(selected_ids,sim):\n",
    "            if s>=0.85:\n",
    "                # preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                # preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                # new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                l,dis = labeled_index.k_nearest_neighbour_I(emb_list[id],average_k)\n",
    "                new_dis_gain = np.mean(dis)\n",
    "\n",
    "                new_gain = max(0, gamma * full_gain[idx_neigh] + lamb * new_dis_gain)\n",
    "\n",
    "                sum_gain += (new_gain-gain[idx_neigh])\n",
    "                gain[idx_neigh] = new_gain\n",
    "\n",
    "    count+=len(keep)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "# return_list contains the added samples to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load\n",
    "The index is pickle serilizable, you can use pickle to save and load the index for next time usage, if not updating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save\n",
    "with open('my_index.pkl', 'wb') as file:\n",
    "    pickle.dump(ann_index, file)\n",
    "\n",
    "#load\n",
    "with open('my_index.pkl', 'rb') as file:\n",
    "    ann_index = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
