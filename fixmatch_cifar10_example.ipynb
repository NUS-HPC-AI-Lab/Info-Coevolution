{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: netstat: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/cifar10/cifar-10-python.tar.gz\n",
      "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
      "lb count: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "ulb count: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "Files already downloaded and verified\n",
      "unlabeled data number: 50000, labeled data number 40\n",
      "Create train and test data loaders\n",
      "[!] data loader keys: dict_keys(['train_lb', 'train_ulb', 'eval'])\n",
      "Create optimizer and scheduler\n",
      "Model loaded\n",
      "Files already downloaded and verified\n",
      "building ann index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:19<00:00, 25.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_index built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import build_index\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.parallel\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from semilearn.algorithms import get_algorithm, name2alg\n",
    "from semilearn.core.utils import ALGORITHMS\n",
    "from semilearn.core.utils import (\n",
    "    TBLog,\n",
    "    count_parameters,\n",
    "    get_logger,\n",
    "    get_net_builder,\n",
    "    get_port,\n",
    "    over_write_args_from_file,\n",
    "    send_model_cuda,\n",
    ")\n",
    "from semilearn.imb_algorithms import get_imb_algorithm, name2imbalg\n",
    "from train import get_config\n",
    "\n",
    "# import clarabel\n",
    "# import qpsolvers\n",
    "\n",
    "import sys\n",
    "import types\n",
    "\n",
    "from typing import Iterator, Optional\n",
    "import torch\n",
    "from torch.utils.data.dataloader import _BaseDataLoaderIter\n",
    "from torch.utils.data import Dataset, _DatasetKind\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from operator import itemgetter\n",
    "import torch.distributed as dist\n",
    "from scipy.sparse import csc_matrix\n",
    "from build_index import *\n",
    "from build_index import confidence_convergence\n",
    "import build_index\n",
    "from typing import Iterable\n",
    "\n",
    "import contextlib\n",
    "import numpy as np\n",
    "from inspect import signature\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, top_k_accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from semilearn.core.hooks import Hook, get_priority, CheckpointHook, TimerHook, LoggingHook, DistSamplerSeedHook, ParamUpdateHook, EvaluationHook, EMAHook, WANDBHook, AimHook\n",
    "from semilearn.core.utils import get_dataset, get_data_loader, get_optimizer, get_cosine_schedule_with_warmup, Bn_Controller\n",
    "from semilearn.core.criterions import CELoss, ConsistencyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "importlib.reload(build_index)\n",
    "\n",
    "sys.argv=['inference.ipynb','--c','config/classic_cv/fixmatch/fixmatch_cifar100_400_0.yaml']\n",
    "\n",
    "def get_config():\n",
    "    from semilearn.algorithms.utils import str2bool\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Semi-Supervised Learning (USB)\")\n",
    "\n",
    "    \"\"\"\n",
    "    Saving & loading of the model.\n",
    "    \"\"\"\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./saved_models\")\n",
    "    parser.add_argument(\"-sn\", \"--save_name\", type=str, default=\"fixmatch\")\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\")\n",
    "    parser.add_argument(\"--load_path\", type=str)\n",
    "    parser.add_argument(\"-o\", \"--overwrite\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\n",
    "        \"--use_tensorboard\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Use tensorboard to plot and save curves\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_wandb\", action=\"store_true\", help=\"Use wandb to plot and save curves\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_aim\", action=\"store_true\", help=\"Use aim to plot and save curves\"\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Training Configuration of FixMatch\n",
    "    \"\"\"\n",
    "    parser.add_argument(\"--epoch\", type=int, default=1)\n",
    "    parser.add_argument(\n",
    "        \"--num_train_iter\",\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help=\"total number of training iterations\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_warmup_iter\", type=int, default=0, help=\"cosine linear warmup iterations\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_eval_iter\", type=int, default=10, help=\"evaluation frequency\"\n",
    "    )\n",
    "    parser.add_argument(\"--num_log_iter\", type=int, default=5, help=\"logging frequency\")\n",
    "    parser.add_argument(\"-nl\", \"--num_labels\", type=int, default=400)\n",
    "    parser.add_argument(\"-bsz\", \"--batch_size\", type=int, default=8)\n",
    "    parser.add_argument(\n",
    "        \"--uratio\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"the ratio of unlabeled data to labeled data in each mini-batch\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\",\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help=\"batch size of evaluation data loader (it does not affect the accuracy)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ema_m\", type=float, default=0.999, help=\"ema momentum for eval_model\"\n",
    "    )\n",
    "    parser.add_argument(\"--ulb_loss_ratio\", type=float, default=1.0)\n",
    "\n",
    "    \"\"\"\n",
    "    Optimizer configurations\n",
    "    \"\"\"\n",
    "    parser.add_argument(\"--optim\", type=str, default=\"SGD\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-2)\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "    parser.add_argument(\n",
    "        \"--layer_decay\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"layer-wise learning rate decay, default to 1.0 which means no layer \"\n",
    "        \"decay\",\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Backbone Net Configurations\n",
    "    \"\"\"\n",
    "    parser.add_argument(\"--net\", type=str, default=\"wrn_28_2\")\n",
    "    parser.add_argument(\"--net_from_name\", type=str2bool, default=False)\n",
    "    parser.add_argument(\"--use_pretrain\", default=False, type=str2bool)\n",
    "    parser.add_argument(\"--pretrain_path\", default=\"\", type=str)\n",
    "\n",
    "    \"\"\"\n",
    "    Algorithms Configurations\n",
    "    \"\"\"\n",
    "\n",
    "    ## core algorithm setting\n",
    "    parser.add_argument(\n",
    "        \"-alg\", \"--algorithm\", type=str, default=\"fixmatch\", help=\"ssl algorithm\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_cat\", type=str2bool, default=True, help=\"use cat operation in algorithms\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--amp\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"use mixed precision training or not\",\n",
    "    )\n",
    "    parser.add_argument(\"--clip_grad\", type=float, default=0)\n",
    "\n",
    "    ## imbalance algorithm setting\n",
    "    parser.add_argument(\n",
    "        \"-imb_alg\",\n",
    "        \"--imb_algorithm\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"imbalance ssl algorithm\",\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Data Configurations\n",
    "    \"\"\"\n",
    "\n",
    "    ## standard setting configurations\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"./data\")\n",
    "    parser.add_argument(\"-ds\", \"--dataset\", type=str, default=\"cifar10\")\n",
    "    parser.add_argument(\"-nc\", \"--num_classes\", type=int, default=10)\n",
    "    parser.add_argument(\"--train_sampler\", type=str, default=\"RandomSampler\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=1)\n",
    "    parser.add_argument(\n",
    "        \"--include_lb_to_ulb\",\n",
    "        type=str2bool,\n",
    "        default=\"True\",\n",
    "        help=\"flag of including labeled data into unlabeled data, default to True\",\n",
    "    )\n",
    "\n",
    "    ## imbalanced setting arguments\n",
    "    parser.add_argument(\n",
    "        \"--lb_imb_ratio\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"imbalance ratio of labeled data, default to 1\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ulb_imb_ratio\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"imbalance ratio of unlabeled data, default to 1\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ulb_num_labels\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"number of labels for unlabeled data, used for determining the maximum \"\n",
    "        \"number of labels in imbalanced setting\",\n",
    "    )\n",
    "\n",
    "    ## cv dataset arguments\n",
    "    parser.add_argument(\"--img_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--crop_ratio\", type=float, default=0.875)\n",
    "\n",
    "    ## nlp dataset arguments\n",
    "    parser.add_argument(\"--max_length\", type=int, default=512)\n",
    "\n",
    "    ## speech dataset algorithms\n",
    "    parser.add_argument(\"--max_length_seconds\", type=float, default=4.0)\n",
    "    parser.add_argument(\"--sample_rate\", type=int, default=16000)\n",
    "\n",
    "    \"\"\"\n",
    "    multi-GPUs & Distributed Training\n",
    "    \"\"\"\n",
    "\n",
    "    ## args for distributed training (from https://github.com/pytorch/examples/blob/master/imagenet/main.py)  # noqa: E501\n",
    "    parser.add_argument(\n",
    "        \"--world-size\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"number of nodes for distributed training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--rank\", default=0, type=int, help=\"**node rank** for distributed training\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-du\",\n",
    "        \"--dist-url\",\n",
    "        default=\"tcp://127.0.0.1:11111\",\n",
    "        type=str,\n",
    "        help=\"url used to set up distributed training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dist-backend\", default=\"nccl\", type=str, help=\"distributed backend\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", default=1, type=int, help=\"seed for initializing training. \"\n",
    "    )\n",
    "    parser.add_argument(\"--gpu\", default=None, type=int, help=\"GPU id to use.\")\n",
    "    parser.add_argument(\n",
    "        \"--multiprocessing-distributed\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Use multi-processing distributed training to launch \"\n",
    "        \"N processes per node, which has N GPUs. This is the \"\n",
    "        \"fastest way to use PyTorch for either single node or \"\n",
    "        \"multi node data parallel training\",\n",
    "    )\n",
    "    # config file\n",
    "    parser.add_argument(\"--c\", type=str, default=\"\")\n",
    "    \n",
    "    parser.add_argument(\"--use-prefetcher\", action=\"store_true\", default=False)\n",
    "\n",
    "    # add algorithm specific parameters\n",
    "    args = parser.parse_args()\n",
    "    over_write_args_from_file(args, args.c)\n",
    "    for argument in name2alg[args.algorithm].get_argument():\n",
    "        parser.add_argument(\n",
    "            argument.name,\n",
    "            type=argument.type,\n",
    "            default=argument.default,\n",
    "            help=argument.help,\n",
    "        )\n",
    "\n",
    "    # add imbalanced algorithm specific parameters\n",
    "    args = parser.parse_args()\n",
    "    over_write_args_from_file(args, args.c)\n",
    "    if args.imb_algorithm is not None:\n",
    "        for argument in name2imbalg[args.imb_algorithm].get_argument():\n",
    "            parser.add_argument(\n",
    "                argument.name,\n",
    "                type=argument.type,\n",
    "                default=argument.default,\n",
    "                help=argument.help,\n",
    "            )\n",
    "    args = parser.parse_args('--c config/classic_cv/fixmatch/fixmatch_cifar10_40_0.yaml'.split())\n",
    "    over_write_args_from_file(args, args.c)\n",
    "    return args\n",
    "\n",
    "args = get_config()\n",
    "port = get_port()\n",
    "args.dist_url = \"tcp://127.0.0.1:\" + str(port)\n",
    "args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.distributed = False\n",
    "args.gpu=0\n",
    "\n",
    "_net_builder = get_net_builder(args.net, args.net_from_name)\n",
    "\n",
    "model = ALGORITHMS['fixmatch']( # name2alg[args.algorithm](\n",
    "            args=args,\n",
    "            net_builder=_net_builder\n",
    "        )\n",
    "\n",
    "model.model = send_model_cuda(args, model.model)\n",
    "model.ema_model = send_model_cuda(args, model.ema_model)\n",
    "# checkpoint = torch.load('latest_model.pth', map_location='cpu')\n",
    "\n",
    "\n",
    "def load_model(self, load_path):\n",
    "    \"\"\"\n",
    "    load model and specified parameters for resume\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(load_path, map_location='cpu')\n",
    "    if not self.distributed and next(iter(checkpoint['model'].keys())).startswith('module.'):\n",
    "        checkpoint['model'] = {k[7:]:v for k,v in checkpoint['model'].items()}\n",
    "    if not self.distributed and next(iter(checkpoint['ema_model'].keys())).startswith('module.'):\n",
    "        checkpoint['ema_model'] = {k[7:]:v for k,v in checkpoint['ema_model'].items()}\n",
    "    self.model.load_state_dict(checkpoint['model'])\n",
    "    self.ema_model.load_state_dict(checkpoint['ema_model'])\n",
    "    self.loss_scaler.load_state_dict(checkpoint['loss_scaler'])\n",
    "    self.it = checkpoint['it']\n",
    "    self.start_epoch = checkpoint['epoch']\n",
    "    self.epoch = self.start_epoch\n",
    "    self.best_it = checkpoint['best_it']\n",
    "    self.best_eval_acc = checkpoint['best_eval_acc']\n",
    "    self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    if self.scheduler is not None and 'scheduler' in checkpoint:\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    self.print_fn('Model loaded')\n",
    "    return checkpoint\n",
    "\n",
    "model.load_model = types.MethodType(load_model, model)\n",
    "model.load_model('continual_fixmatch_cifar10_add500_to_500_1400/model_best.pth')\n",
    "model.gpu=0\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],)\n",
    "    ])\n",
    "cifar10_dataset = torchvision.datasets.CIFAR10('./data',train=True, download=True,transform=transform_val)\n",
    "cifar10_dataloader = DataLoader(cifar10_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "ann_index = build_index.Singlemodal_index(dim=128,n=50000,submodular_k=16,num_classes=10)\n",
    "\n",
    "cifar10_dataloader = DataLoader(cifar10_dataset, batch_size=100, shuffle=False)\n",
    "model.cifar_train_loader = cifar10_dataloader\n",
    "\n",
    "def build_ann_index(self, eval_dest='train_ulb', out_key='logits', return_logits=False):\n",
    "    \"\"\"\n",
    "    evaluation function\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "    # self.ema.apply_shadow()\n",
    "\n",
    "    # eval_loader = self.loader_dict[eval_dest]\n",
    "    data_loader = self.cifar_train_loader\n",
    "    total_loss = 0.0\n",
    "    total_num = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "    y_logits = []\n",
    "    confidence_list = []\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(data_loader):\n",
    "            if isinstance(x, dict):\n",
    "                x = {k: v.cuda(self.gpu) for k, v in x.items()}\n",
    "            else:\n",
    "                x = x.cuda(self.gpu)\n",
    "            y = y.cuda(self.gpu)\n",
    "\n",
    "            num_batch = y.shape[0]\n",
    "            total_num += num_batch\n",
    "\n",
    "            # out = self.model(x)\n",
    "            out = self.ema_model(x)\n",
    "            \n",
    "            logits = out[out_key]\n",
    "            prob = torch.softmax(logits, dim=-1)\n",
    "            feat = out['feat'].detach().cpu()\n",
    "            pred = torch.max(logits, dim=-1)[1].cpu()\n",
    "            conf = (prob.max(dim=-1)[0]-0.1)/0.9\n",
    "            \n",
    "            for f,label,confid in zip(feat, pred.detach().cpu(),conf.detach().cpu()):\n",
    "                ann_index.add_item(build_index.DataPoint(None,f,label,confid))\n",
    "            loss = F.cross_entropy(logits, y, reduction='mean', ignore_index=-1)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "            y_logits.append(logits.cpu().numpy())\n",
    "            y_probs.extend(prob.cpu().tolist())\n",
    "            total_loss += loss.item() * num_batch\n",
    "            \n",
    "            \n",
    "            confidence_list.extend(conf.cpu().tolist())\n",
    "    \n",
    "    return confidence_list, y_true, y_pred\n",
    "\n",
    "\n",
    "model.build_ann_index = types.MethodType(build_ann_index,model)\n",
    "\n",
    "print('building ann index')\n",
    "conf, y_t, y_p = model.build_ann_index()\n",
    "print('ann_index built')\n",
    "\n",
    "# print(len(ann_index.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reannotate_gain(relabel_confidence, use_ann_index=False,num_classes=10):\n",
    "    #try directly using confidence instead of making it into entropy, so that the cosine space is more reasonable\n",
    "    gain_list = np.zeros(50000)\n",
    "    c_list = np.zeros(50000)\n",
    "    p_list = np.zeros(50000)\n",
    "    for idx in tqdm(range(50000)):\n",
    "        if ann_index.data[idx].confidence<relabel_confidence:\n",
    "            if use_ann_index and ann_index is not None:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx], k=8, skip_one=True)\n",
    "                preds_model = (ann_index.data[idx].label, ann_index.data[idx].confidence)\n",
    "                merged_p,c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                p_list[idx] = merged_p\n",
    "\n",
    "            else:\n",
    "                c = ann_index.data[idx].confidence\n",
    "                p_list[idx] = ann_index.data[idx].label\n",
    "\n",
    "            gain_list[idx] = max(0, relabel_confidence-c)\n",
    "            c_list[idx] = c\n",
    "        else:\n",
    "            c_list[idx] = 1\n",
    "            p_list[idx] = ann_index.data[idx].label\n",
    "            continue\n",
    "    # here return positive gains in ablation\n",
    "    return gain_list, c_list, p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:13<00:00, 3842.79it/s]\n"
     ]
    }
   ],
   "source": [
    "expected_confidence=1\n",
    "full_gain,c_list, p_list = reannotate_gain(expected_confidence,True)\n",
    "# print('non zero gain number',len(np.nonzero(gain)[0]))\n",
    "# print(sorted(gain[np.nonzero(gain)[0]].tolist()))\n",
    "non_zero_gain_idx = np.nonzero(full_gain)[0]\n",
    "# emb_list = np.array([ann_index.data[idx].I_feature for idx in non_zero_gain_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0638e+00,  3.7323e+00,  8.2429e-01,  1.9195e+00,  5.2162e+00,\n",
       "         2.3690e+00,  7.9272e-01,  2.9340e+00,  6.4577e+00,  1.0876e+00,\n",
       "         5.3472e+00,  5.3533e+00,  2.4846e+00,  4.8310e-01,  6.4177e-01,\n",
       "        -1.9481e-01,  8.7279e-01,  3.3311e+00,  6.7676e+00,  4.9254e+00,\n",
       "         5.3077e-01,  5.9769e+00, -1.9307e-01,  1.4010e+00,  4.0692e+00,\n",
       "         5.0716e+00,  3.2406e+00,  3.5962e+00,  5.9761e+00, -6.4964e-02,\n",
       "        -1.0025e-01,  2.1499e+00,  2.2794e-01,  4.3380e-01,  5.5643e-01,\n",
       "         5.8197e+00,  3.0632e+00,  2.5042e+00,  1.5730e+00,  2.2729e+00,\n",
       "         2.8092e+00,  5.7527e+00,  4.9005e+00, -2.3075e-01,  3.1336e-01,\n",
       "         9.3917e-01,  4.0123e+00,  1.8686e-01,  9.9656e-02,  1.3687e-01,\n",
       "         4.4088e-01,  1.2722e+00,  4.4434e+00,  1.8519e-01,  1.9006e+00,\n",
       "         1.4828e-01,  2.6330e-01,  7.4683e-01, -1.8756e-01,  2.0648e-03,\n",
       "         4.6652e-01, -1.4679e-02,  4.1223e+00,  7.6769e-02,  1.6504e-01,\n",
       "         5.1360e+00, -1.2434e-01,  5.1920e-02, -2.8435e-02,  3.2308e-01,\n",
       "         6.4773e+00,  1.4337e+00,  2.5285e+00,  1.0646e+00,  4.6256e+00,\n",
       "         5.2378e+00,  6.6827e-01,  4.1194e+00,  4.1682e-02,  5.6789e+00,\n",
       "         1.2375e-01,  6.3176e+00,  4.5215e-01,  3.2160e+00,  3.9056e+00,\n",
       "         1.3478e+00,  4.0026e-02,  4.4611e-02,  2.8788e+00,  4.0306e+00,\n",
       "         1.5663e-01,  2.0675e-01,  1.2698e+00, -1.0531e-01,  4.5096e+00,\n",
       "         5.6671e+00, -1.9195e-01,  1.3576e-01,  1.2703e+00,  3.8711e+00,\n",
       "         1.8431e+00,  1.3966e+00,  5.7246e-02,  1.1934e-02,  6.2661e+00,\n",
       "         1.9460e+00,  2.2280e+00,  1.8697e+00, -1.7939e-01,  2.9511e+00,\n",
       "         5.2889e-01,  7.9445e-02,  2.8482e+00,  2.4847e+00,  1.7297e+00,\n",
       "         8.4115e-02, -1.8418e-01,  6.1384e-01,  3.0771e+00, -2.2711e-01,\n",
       "         4.6742e+00,  1.8543e+00,  4.2157e+00,  3.0803e-01,  3.6542e+00,\n",
       "         2.8611e-01,  3.7845e-01, -9.0702e-02])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_index.data[1].I_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_list = np.stack([data.I_feature for data in ann_index.data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np.random.choice baseline 随机选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_samples = [41044,20892,20660,9228,21975,42229,4271,28845,18464, 22901,\n",
    "  33517, 1726,26194, 49944, 14070, 16655, 31077, 8224,\n",
    "  1949, 32060, 45898, 2738, 31227, 11017, 33096, 47157,\n",
    "  34363, 41449, 28579, 21458, 35040, 22122, 44931, 13857,\n",
    "  29046, 18920, 4799, 44263, 31903, 46107, 9959, 41442, \n",
    "  49523, 29231,  8548, 29083, 38689,  3849, 46568,\n",
    "  40878, 11766, 15412,  4259,  2803, 32791,  6082, 22287,  5725,\n",
    "  24817,  4231, 29442, 30134, 19606, 14942, 15330, 31440,  6207,\n",
    "  49077, 46513, 39905, 30715,  9258, 16038, 21485, 20140,  3587,\n",
    "  29755,  1499, 27685, 39228, 36052, 16914, 48245, 10971, 10512,\n",
    "  13803, 44581, 38286, 23389, 44788, 32732, 23343, 22460, 10242,\n",
    "  33312, 10950,  8485,  5996, 36392, 27392,  3837, 19135, 20325,\n",
    "  41041,  7853,  1801, 45819, 10857,  9842, 24662,  7865, 45931,\n",
    "  12153,  2130, 36083,  2787, 10875, 46297,  7997,  5749, 32685,\n",
    "  38916, 37621, 29795,   602, 40366, 44782, 43366, 42862, 12508,\n",
    "  13434,  6037, 10502, 17404,  3763,   901, 17712, 23044,  9820,\n",
    "  11166,  8802, 21841, 33428, 14919, 18167, 35318, 23233, 45854,\n",
    "  25118, 11699, 19154, 35265, 14034,  6319,  9008,  2457, 35593,\n",
    "  19130, 24565, 15949, 24842, 24628, 22893, 27136, 20139, 47160,\n",
    "  3989, 39318, 38517, 26081, 10542, 37613, 18790, 46923, 21951,\n",
    "  16522, 15257, 40719, 29971, 24130, 38684, 18617, 42401, 47475,\n",
    "  10495, 12689, 37567, 41550, 25504, 23859, 27259,  5347, 12156,\n",
    "  12687, 34461, 35151, 12966,  3932, 35635, 49544, 49614, 28315,\n",
    "  16733, 23413, 24100, 23012, 10115, 35044, 20920,  8395,\n",
    "  17919, 25428, 36692, 42365, 37983, 43915, 35584, 39211, 24859,\n",
    "  2387, 24174, 18450,  1124, 14161,   712, 13905, 44516,    17,\n",
    "    717, 35109, 23261, 29265, 31161, 12147, 21487, 13825, 12821,\n",
    "  34197, 29087, 25207, 34428, 26890, 36511,  9841, 14820, 34345,\n",
    "  25787, 46899, 49953, 37514, 49492, 22720,  3546, 23788, 26008,\n",
    "  43546,  7099, 36425, 16157, 45619, 43576,  8080,  1793, 26559,\n",
    "  43730, 32558, 11873, 45746,  6324,  1282, 20514, 30735, 21926,\n",
    "  7223, 32322, 19857, 21787, 41892, 32818, 45330,  4319, 25997,\n",
    "  5108, 46541, 32717, 12993, 36738, 40846,  3679, 23128, 34754,\n",
    "  36645, 37038, 25934, 33632, 13307, 13687,  1705,  5581, 24779,\n",
    "  34218,  8760, 11528, 33463, 36518,  5744, 37767, 36870, 37369,\n",
    "  27118,  5147, 46707,  4025, 34626, 36554, 31823, 11730, 30660,\n",
    "  13618, 13673,  5867, 39220, 27540, 25311, 29196, 13733, 21211,\n",
    "  45467, 47194, 33714, 36138, 21525, 39898, 44532, 27173,  8173,\n",
    "  14289, 30836, 14244, 46245, 14929, 41478, 37310, 13012, 11663,\n",
    "  41610, 15224, 14171, 26979, 20117, 31887, 42219, 21533, 47591,\n",
    "  27560, 14838, 28834, 31030, 36292, 38445, 47933, 36403, 34895,\n",
    "  19738, 49117, 43793, 26911, 22453, 15916,  2867, 29451, 15411,\n",
    "  27144,   283, 39718, 46089, 12332, 29446, 40954, 44029, 10849,\n",
    "  18222,  8918, 38528, 44717, 29198, 19712, 19649, 41377, 46475,\n",
    "  6343,  7207, 47325, 44572, 18824, 27167, 26907, 35921, 33202,\n",
    "  9803, 40659,  4220, 46650,  3454, 14614, 32216, 41804, 46570,\n",
    "  26860,  1630, 34189, 47534, 34015, 47632, 44802, 48796,  7533,\n",
    "  49666, 18449, 29025,  9790,  1744, 31337, 32956, 10482,  9798,\n",
    "  38971, 19868, 10518, 22218, 24337, 40973, 41805, 30507, 26823,\n",
    "  21753, 42999, 27753, 30602, 21739, 40389, 14692, 13089,  3738,\n",
    "  3125, 10720, 19637,  3902, 48878, 27743, 41488, 32826, 22923,\n",
    "  42881, 36905, 17003,  8347, 32055, 38025, 47892]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_gain[selected_samples]=0\n",
    "num = 500\n",
    "return_list = np.random.choice(list(range(50000)), num,replace=False, p=full_gain/sum(full_gain),replace=False)\n",
    "print(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Selection Process with Neighbour-Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.727950096130371\n"
     ]
    }
   ],
   "source": [
    "# ann作为初始集：\n",
    "import time\n",
    "count = 0\n",
    "labeled_set = set(selected_samples)\n",
    "return_list = set()\n",
    "gain = full_gain.copy()\n",
    "sum_gain = sum(gain)\n",
    "start = time.time()\n",
    "while count<500 and sum_gain>1:\n",
    "    # print(sum(gain),sum_gain)\n",
    "    idx = np.random.choice(50000,p=gain/sum(gain))\n",
    "    sum_gain -= gain[idx]\n",
    "    gain[idx] = 0\n",
    "    if idx in return_list or idx in labeled_set: \n",
    "        continue\n",
    "    \n",
    "    relabel_y = y_t[idx]\n",
    "    ann_index.data[idx].label = relabel_y\n",
    "    ann_index.data[idx].confidence = 1\n",
    "    return_list.add(idx)\n",
    "    \n",
    "    I_near_labels, I_near_distances = ann_index.k_nearest_neighbour_I(ann_index.data[idx], 8, skip_one=True)\n",
    "    selected_ids = I_near_labels[I_near_distances<=0.1]\n",
    "    sim = 1-I_near_distances[I_near_distances<=0.1]\n",
    "    classes = np.array([ann_index.data[idx].label for idx in selected_ids])\n",
    "    # confidences = np.array([ann_index.data[idx].confidence for idx in selected_ids])\n",
    "    \n",
    "    for idx_neigh,s, cls in zip(selected_ids,sim,classes):\n",
    "        if cls==relabel_y:\n",
    "            if s>0.85:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                new_gain = max(0, 1-new_c)\n",
    "                sum_gain += (new_gain-gain[idx_neigh])\n",
    "                gain[idx_neigh] = new_gain\n",
    "        else:\n",
    "            if s>0.85:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                new_gain = max(gain[idx_neigh], 1-new_c)\n",
    "                sum_gain += (new_gain-gain[idx_neigh])\n",
    "                gain[idx_neigh] = new_gain\n",
    "                # print('found conflicting neighbour!')\n",
    "    count+=1\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{34818, 8196, 16392, 34835, 2069, 22552, 40989, 24605, 2078, 30753, 26660, 38, 6184, 28712, 22576, 24627, 36922, 4157, 45118, 49218, 26692, 24645, 34889, 20558, 85, 41056, 32864, 4198, 39021, 39024, 20593, 34928, 2162, 30838, 30839, 22648, 20601, 10361, 41083, 39037, 24704, 30848, 24708, 16519, 141, 32923, 49307, 28835, 39092, 12469, 39100, 41148, 39104, 45251, 39112, 22729, 22731, 20690, 28883, 35026, 47322, 8410, 232, 47349, 33016, 37117, 30976, 22785, 35072, 259, 49419, 12563, 16662, 49440, 45348, 24877, 18739, 16694, 35129, 10554, 43328, 12616, 33099, 49484, 6490, 10587, 24924, 4456, 12649, 29040, 20849, 49521, 20850, 10620, 33152, 47492, 39301, 14724, 27021, 401, 39325, 37278, 22943, 39329, 4519, 25000, 31144, 14774, 14775, 39355, 35266, 4547, 10694, 39366, 33223, 14800, 39378, 41427, 22996, 23003, 41436, 16859, 49632, 33250, 27107, 43495, 35311, 45577, 33313, 8738, 12837, 45606, 18986, 10798, 49710, 39474, 33333, 25142, 39477, 12853, 19007, 14918, 6729, 23114, 31308, 599, 6747, 47710, 23136, 45665, 35429, 25191, 37485, 27246, 41583, 43635, 14965, 6782, 640, 23174, 2695, 49826, 25252, 10916, 39591, 12970, 15020, 686, 33459, 43701, 8887, 47801, 31421, 6853, 43720, 43722, 31439, 13007, 41681, 37586, 21208, 2776, 8922, 41692, 43742, 8931, 13030, 29422, 47860, 17141, 17144, 11006, 31495, 47880, 41737, 19210, 41740, 13069, 37644, 47893, 17175, 15131, 13085, 804, 31527, 810, 13100, 31536, 9009, 39731, 19252, 21308, 35645, 31550, 6979, 9035, 25420, 4940, 35665, 19285, 2907, 29532, 45916, 13150, 41824, 33638, 43886, 27505, 23417, 13177, 11137, 29570, 45956, 11142, 5000, 13192, 9101, 48019, 13204, 19350, 35735, 923, 13223, 7090, 29618, 29620, 37815, 25527, 25528, 27578, 41915, 11199, 13248, 15305, 23498, 27596, 46039, 7130, 993, 39906, 31714, 7159, 27640, 27641, 46091, 15371, 19483, 1057, 17451, 9263, 31795, 3124, 27702, 25656, 17467, 15419, 3133, 33854, 35903, 35906, 11330, 42053, 40009, 21582, 9295, 21587, 15444, 29780, 15450, 21597, 15468, 48243, 25716, 38011, 35963, 9343, 33921, 38022, 33931, 44175, 13465, 25754, 9371, 3234, 17572, 5296, 29874, 17589, 5304, 1211, 46268, 27838, 38080, 9417, 13532, 27873, 15585, 38115, 25826, 31974, 27887, 7408, 13552, 34038, 29959, 38163, 15646, 29982, 46373, 27950, 13616, 5425, 17714, 17720, 7493, 25932, 7509, 25941, 13656, 32090, 1370, 11617, 3428, 30055, 36200, 42345, 46443, 5494, 25981, 38274, 28035, 21892, 32133, 30095, 34203, 40350, 46496, 23971, 17840, 47103, 11710, 26052, 30151, 34250, 24011, 15820, 32208, 38352, 7635, 38364, 32225, 17889, 40419, 1511, 38380, 9710, 19950, 1524, 48629, 32245, 22008, 34299, 19972, 42502, 3592, 40467, 30229, 11799, 30234, 17953, 38437, 9767, 42541, 44590, 17965, 32306, 42546, 5684, 24118, 15937, 20046, 22095, 24147, 46686, 13925, 48741, 9831, 15977, 13933, 48750, 5747, 30337, 44674, 28294, 38534, 16006, 16009, 30346, 11919, 34465, 13986, 11938, 40615, 48814, 34492, 32446, 44736, 14021, 7879, 18127, 22228, 34516, 24284, 24289, 34538, 16109, 42738, 22258, 5876, 34550, 20215, 7926, 46853, 28433, 46869, 5915, 42783, 40739, 46886, 30505, 24365, 10036, 3900, 12096, 24388, 18248, 22346, 8019, 16219, 42845, 28510, 10080, 44897, 12132, 49002, 49003, 12141, 10095, 32624, 20335, 8054, 16252, 34686, 26500, 30636, 4014, 4016, 26545, 12210, 49079, 42935, 1977, 24506, 18363, 16316, 26551, 42945, 34756, 22469, 20426, 2000, 12264, 36843, 2028, 49132, 26609, 45046, 6135, 26616, 49151}\n"
     ]
    }
   ],
   "source": [
    "print(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Selection with larger batch step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809117317199707\n"
     ]
    }
   ],
   "source": [
    "# ann作为初始集：\n",
    "import time\n",
    "selection_batchsize = 100\n",
    "count = 0\n",
    "labeled_set = set(selected_samples)\n",
    "return_list = set()\n",
    "gain = full_gain.copy()\n",
    "sum_gain = sum(gain)\n",
    "start = time.time()\n",
    "target_count = 500\n",
    "while count<target_count and sum_gain>1:\n",
    "    \n",
    "    # print(sum(gain),sum_gain)\n",
    "    \n",
    "    idxs = np.random.choice(50000,min(selection_batchsize,target_count-count),p=gain/sum(gain),replace=False)\n",
    "    keep = []\n",
    "    for idx in idxs:\n",
    "        sum_gain -= gain[idx]\n",
    "        gain[idx] = 0\n",
    "        if idx in return_list or idx in labeled_set: \n",
    "            continue\n",
    "        else:\n",
    "            keep.append(idx)\n",
    "            \n",
    "    \n",
    "    for idx in keep:\n",
    "        relabel_y = y_t[idx]\n",
    "        ann_index.data[idx].label = relabel_y\n",
    "        ann_index.data[idx].confidence = 1\n",
    "        return_list.add(idx)\n",
    "\n",
    "        I_near_labels, I_near_distances = ann_index.k_nearest_neighbour_I(ann_index.data[idx], 8, skip_one=True)\n",
    "        selected_ids = I_near_labels[I_near_distances<=0.1]\n",
    "        sim = 1-I_near_distances[I_near_distances<=0.1]\n",
    "        classes = np.array([ann_index.data[idx].label for idx in selected_ids])\n",
    "        # confidences = np.array([ann_index.data[idx].confidence for idx in selected_ids])\n",
    "\n",
    "        for idx_neigh,s, cls in zip(selected_ids,sim,classes):\n",
    "            if cls==relabel_y:\n",
    "                if s>0.85:\n",
    "                    preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                    preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                    new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                    new_gain = max(0, 1-new_c)\n",
    "                    sum_gain += (new_gain-gain[idx_neigh])\n",
    "                    gain[idx_neigh] = new_gain\n",
    "            else:\n",
    "                if s>0.85:\n",
    "                    preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                    preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                    new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                    new_gain = max(gain[idx_neigh], 1-new_c)\n",
    "                    sum_gain += (new_gain-gain[idx_neigh])\n",
    "                    gain[idx_neigh] = new_gain\n",
    "                    # print('found conflicting neighbour!')\n",
    "    count+=len(keep)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{34818, 8196, 16392, 2069, 22552, 40989, 2078, 30753, 26660, 38, 28712, 6184, 24627, 36922, 4157, 49218, 34889, 20558, 85, 39021, 39024, 10361, 20601, 39037, 30848, 24708, 16519, 141, 24723, 32923, 49307, 28835, 39092, 12469, 41148, 39100, 45251, 39112, 22729, 22731, 20690, 28883, 8410, 47322, 47349, 30976, 259, 49419, 12563, 49440, 45348, 24877, 18739, 16694, 35129, 10554, 12616, 33099, 6490, 10587, 24924, 4456, 12649, 20849, 49521, 33152, 47492, 39301, 27021, 39325, 37278, 22943, 39329, 4519, 25000, 14774, 14775, 35266, 10694, 33223, 39378, 41427, 22996, 41436, 27107, 45606, 18986, 10798, 49710, 33333, 25142, 12853, 19007, 14918, 6729, 23114, 6747, 47710, 35429, 25191, 37485, 41583, 43635, 14965, 6782, 640, 23174, 2695, 25252, 10916, 39591, 12970, 15020, 686, 33459, 8887, 6853, 43720, 41681, 37586, 21208, 2776, 43742, 8931, 29422, 47860, 17141, 31495, 47880, 41737, 19210, 41740, 804, 31527, 9009, 35645, 31550, 6979, 9035, 4940, 25420, 2907, 33638, 27505, 13177, 23417, 45956, 5000, 9101, 48019, 13204, 923, 41883, 7090, 29620, 25527, 37815, 25528, 27578, 41915, 11199, 13248, 15305, 27596, 46039, 7159, 27640, 27641, 46091, 1057, 17451, 31795, 3124, 27702, 17467, 3133, 33854, 35903, 35906, 42053, 40009, 21582, 29780, 15450, 21597, 15468, 48243, 25716, 38011, 9343, 33921, 38022, 33931, 44175, 13465, 25754, 9371, 17589, 5304, 1211, 38080, 9417, 13532, 27873, 15585, 38115, 31974, 27887, 7408, 34038, 29959, 38163, 15646, 46373, 13616, 17720, 7493, 7509, 32090, 30055, 36200, 42345, 46443, 5494, 25981, 38274, 28035, 21892, 32133, 30095, 34203, 40350, 23971, 17840, 11710, 26052, 30151, 15820, 7635, 32225, 38380, 9710, 1524, 48629, 49151, 19972, 42502, 3592, 11799, 17953, 38437, 9767, 17965, 44590, 42546, 32306, 5684, 24118, 15937, 11847, 20046, 22095, 48741, 9831, 15977, 48750, 5747, 30337, 44674, 38534, 28294, 16009, 11919, 34465, 40615, 32446, 14021, 7879, 18127, 22228, 24284, 24289, 34538, 42738, 5876, 34550, 20215, 28433, 46869, 5915, 42783, 40739, 3900, 24388, 18248, 22346, 8019, 28510, 44897, 49002, 12141, 32624, 16252, 26500, 30636, 4014, 4016, 12210, 42935, 49079, 1977, 24506, 18363, 16316, 42945, 22469, 2000, 36843, 2028, 6135, 47103}\n"
     ]
    }
   ],
   "source": [
    "print(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further add gain of distance to labeled and high confidence labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_k = 4 # hyp to control neighbour number to estimate the distance gain\n",
    "labeled_set = set(selected_samples)\n",
    "labeled_index = build_index.Singlemodal_index(dim=128,n=50000,submodular_k=8,num_classes=10)\n",
    "conf = np.array(conf)\n",
    "high_conf_set = np.where(conf>0.95)[0]\n",
    "cleaner_set = set(high_conf_set).union(labeled_set)\n",
    "for id in cleaner_set:\n",
    "    labeled_index.add_item(build_index.DataPoint(None,emb_list[id],y_t[id],conf[id]))\n",
    "dis_gain = np.zeros(50000)\n",
    "for id in (set(range(50000))-cleaner_set):\n",
    "    l,dis = labeled_index.k_nearest_neighbour_I(emb_list[id],average_k)\n",
    "    dis_gain[id] = np.mean(dis)\n",
    "dis_gain = dis_gain/(np.max(dis_gain)+1e-9) # normlize the impact of this gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann作为初始集：\n",
    "lamb = 1 # the hyp to control balance of distance gain and confidence gain. Tune between 0.0 and 1\n",
    "count = 0\n",
    "return_list = set()\n",
    "gain = full_gain+lamb * dis_gain\n",
    "while count<500 and sum(gain)>1:\n",
    "    \n",
    "    idx = np.random.choice(list(range(50000)),p=gain/sum(gain),replace=False)\n",
    "    gain[idx] = 0\n",
    "    \n",
    "    if idx in return_list or idx in labeled_set: continue\n",
    "    \n",
    "    relabel_y = y_t[idx]\n",
    "    ann_index.data[idx].label = relabel_y\n",
    "    ann_index.data[idx].confidence = 1\n",
    "    return_list.add(idx)\n",
    "    \n",
    "    I_near_labels, I_near_distances = ann_index.k_nearest_neighbour_I(ann_index.data[idx], 8, skip_one=True)\n",
    "    selected_ids = I_near_labels[I_near_distances<=0.1]\n",
    "    sim = 1-I_near_distances[I_near_distances<=0.1]\n",
    "    classes = np.array([ann_index.data[idx].label for idx in selected_ids])\n",
    "    # confidences = np.array([ann_index.data[idx].confidence for idx in selected_ids])\n",
    "    \n",
    "    for idx_neigh,s, cls in zip(selected_ids,sim,classes):\n",
    "        if cls==relabel_y:\n",
    "            if s>0.85:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                gain[idx_neigh] = max(0, 1-new_c)\n",
    "        else:\n",
    "            if s>0.85:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                gain[idx_neigh] = max(gain[idx_neigh], 1-new_c)\n",
    "                # print('found conflicting neighbour!')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New version of adding gain of distance to labeled and high confidence labels (For extending semi-supervised unlabeled data)\n",
    "\n",
    "Equation should be: dis_to_labeled + dis_to_selected_unlabeled. that is to say, once we get a batch of midpoints, then we uniformly expand the boundarys. One way is to add selected_unlabeled into distance index and estimate the gain.\n",
    "\n",
    "Another term: for samples higher than 0.5 but lower than 0.9 acc, they are pesudo labeled actually. We may need these samples for middle stage learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.016977593"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_k = 4 # hyp to control neighbour number to estimate the distance gain\n",
    "labeled_set = set(selected_samples) # change it to 10000\n",
    "labeled_index = build_index.Singlemodal_index(dim=128,n=50000,submodular_k=8,num_classes=10)\n",
    "conf = np.array(conf)\n",
    "high_conf_set = np.where(conf>0.9)[0]\n",
    "cleaner_set = set(high_conf_set).union(labeled_set)\n",
    "for id in cleaner_set:\n",
    "    labeled_index.add_item(build_index.DataPoint(None,emb_list[id],None,0)) # 扩展集数据的embeding和对应index注意处理，可以拼接起来\n",
    "dis_gain = np.zeros(50000)\n",
    "for id in (set(range(50000))-cleaner_set):\n",
    "    l,dis = labeled_index.k_nearest_neighbour_I(emb_list[id],average_k)\n",
    "    dis_gain[id] = np.mean(dis)\n",
    "dis_gain = dis_gain/(np.max(dis_gain)+1e-9) # normlize the impact of this gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4113731384277344\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# ann作为初始集：\n",
    "lamb = 1 # the hyp to control balance of distance gain and confidence gain. For extended dataset, \n",
    "         # we do not need to tune this as previous experiments show that lamb=1 is more stable for semi-supervised setting.\n",
    "gamma = 1 # set 0 or 1, lets see whether to turn off this\n",
    "count = 0\n",
    "return_list = set()\n",
    "gain = full_gain.copy()\n",
    "selection_batchsize = 100\n",
    "### For semi-supervised learning with threshold 0.5, this line emphasize those semi-labeled data. \n",
    "gain[gain>0.5] = 0 \n",
    "### Try using this line and without this line. NOTE: For extending supervised data, comment it out.\n",
    "\n",
    "gain = gamma * gain + lamb * dis_gain\n",
    "sum_gain = sum(gain)\n",
    "start = time.time()\n",
    "target_count = 500\n",
    "while count<target_count and sum_gain>1:\n",
    "    # print(sum(gain),sum_gain)\n",
    "    idxs = np.random.choice(50000,min(selection_batchsize,target_count-count),p=gain/sum(gain),replace=False)\n",
    "    keep = []\n",
    "    for idx in idxs:\n",
    "        sum_gain -= gain[idx]\n",
    "        gain[idx] = 0\n",
    "        if idx in return_list or idx in labeled_set: \n",
    "            continue\n",
    "        else:\n",
    "            keep.append(idx)\n",
    "            \n",
    "    \n",
    "    for idx in keep:\n",
    "        relabel_y = y_t[idx]\n",
    "        ann_index.data[idx].label = relabel_y\n",
    "        ann_index.data[idx].confidence = 1\n",
    "        return_list.add(idx)\n",
    "\n",
    "        I_near_labels, I_near_distances = ann_index.k_nearest_neighbour_I(ann_index.data[idx], 8, skip_one=True)\n",
    "        selected_ids = I_near_labels[I_near_distances<=0.15]\n",
    "        sim = 1-I_near_distances[I_near_distances<=0.15]\n",
    "        # classes = np.array([ann_index.data[idx].label for idx in selected_ids])\n",
    "        # confidences = np.array([ann_index.data[idx].confidence for idx in selected_ids])\n",
    "\n",
    "        labeled_index.add_item(build_index.DataPoint(None,emb_list[id],None,0))\n",
    "        \n",
    "        for idx_neigh,s in zip(selected_ids,sim):\n",
    "            if s>=0.85:\n",
    "                # preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                # preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                # new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                l,dis = labeled_index.k_nearest_neighbour_I(emb_list[id],average_k)\n",
    "                new_dis_gain = np.mean(dis)\n",
    "\n",
    "                new_gain = max(0, gamma * full_gain[idx_neigh] + lamb * new_dis_gain)\n",
    "\n",
    "                sum_gain += (new_gain-gain[idx_neigh])\n",
    "                gain[idx_neigh] = new_gain\n",
    "\n",
    "    count+=len(keep)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also dynamic rechecking the bayesian gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also dynamic rechecking the bayesian gain\n",
    "\n",
    "import time\n",
    "# ann作为初始集：\n",
    "lamb = 1 # the hyp to control balance of distance gain and confidence gain. For extended dataset, \n",
    "         # we do not need to tune this as previous experiments show that lamb=1 is more stable for semi-supervised setting.\n",
    "gamma = 1 # set 0 or 1, lets see whether to turn off this\n",
    "count = 0\n",
    "return_list = set()\n",
    "gain = full_gain.copy()\n",
    "selection_batchsize = 100\n",
    "### For semi-supervised learning with threshold 0.5, this line emphasize those semi-labeled data. \n",
    "gain[gain>0.5] = 0 \n",
    "### Try using this line and without this line. NOTE: For extending supervised data, comment it out.\n",
    "\n",
    "gain = gamma * gain + lamb * dis_gain\n",
    "sum_gain = sum(gain)\n",
    "start = time.time()\n",
    "target_count = 500\n",
    "while count<target_count:\n",
    "    # print(sum(gain),sum_gain)\n",
    "    idxs = np.random.choice(50000,min(selection_batchsize,target_count-count),p=gain/sum(gain),replace=False)\n",
    "    keep = []\n",
    "    for idx in idxs:\n",
    "        sum_gain -= gain[idx]\n",
    "        gain[idx] = 0\n",
    "        if idx in return_list or idx in labeled_set: \n",
    "            continue\n",
    "        else:\n",
    "            keep.append(idx)\n",
    "            \n",
    "    \n",
    "    for idx in keep:\n",
    "        relabel_y = y_t[idx]\n",
    "        ann_index.data[idx].label = relabel_y\n",
    "        ann_index.data[idx].confidence = 1\n",
    "        return_list.add(idx)\n",
    "\n",
    "        I_near_labels, I_near_distances = ann_index.k_nearest_neighbour_I(ann_index.data[idx], 8, skip_one=True)\n",
    "        selected_ids = I_near_labels[I_near_distances<=0.15]\n",
    "        sim = 1-I_near_distances[I_near_distances<=0.15]\n",
    "        # classes = np.array([ann_index.data[idx].label for idx in selected_ids])\n",
    "        # confidences = np.array([ann_index.data[idx].confidence for idx in selected_ids])\n",
    "\n",
    "        labeled_index.add_item(build_index.DataPoint(None,emb_list[id],None,0))\n",
    "        \n",
    "        for idx_neigh,s in zip(selected_ids,sim):\n",
    "            if s>=0.85:\n",
    "                preds_dataset = ann_index.knn_pred(ann_index.data[idx_neigh], k=8, skip_one=True)\n",
    "                preds_model = ann_index.data[idx_neigh].label,ann_index.data[idx_neigh].confidence\n",
    "                new_p,new_c = confidence_convergence(preds_dataset,preds_model,conf_decay=True)\n",
    "                gain_bayesian = max(0,1-new_c)\n",
    "\n",
    "                l,dis = labeled_index.k_nearest_neighbour_I(emb_list[id],average_k)\n",
    "                new_dis_gain = np.mean(dis)\n",
    "\n",
    "                new_gain = max(0, gamma * gain_bayesian + lamb * new_dis_gain)\n",
    "\n",
    "                sum_gain += (new_gain-gain[idx_neigh])\n",
    "                gain[idx_neigh] = new_gain\n",
    "\n",
    "    count+=len(keep)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
